---
- name: Deploy DeepSeek R1 Model
  shell: |
    kollama undeploy {{ item  }} -n {{ _deepseek.namespace }}
  register: deploy_dsr1_model
  args:
    executable: /bin/bash
    chdir: "{{ base_path }}"
  environment:
    PATH: "/root/go/bin:{{ ansible_env.PATH }}"
  with_items:
    - deepseek-r1
    # - phi
    # - phi4
- debug: msg={{ deploy_dsr1_model }}
  when: print_debug == true


# ollama-operator-system   pod/ollama-model-phi-74759f7d47-tbp8c                     0/1     Init:0/1   0          62s
# ollama-operator-system   pod/ollama-models-store-0                                 1/1     Running    0          2m18s
# ollama-operator-system   pod/ollama-operator-controller-manager-74d7dccc76-4hl6v   2/2     Running    0          7m


# kollama deploy phi -n ollama-operator-system --expose --service-type=LoadBalancer
# Deploying model "phi"...
#
# ‚úì image store is ready
# ‚úì image store exposed
# ‚†π pulling model image "phi"...
#
# ‚†º pulling model image "phi"...
# ‚†ß pulling model image "phi"...
# ‚†ã pulling model image "phi"...
# ‚úì model pulled and prepared
# ‚úì model is ready
# ‚úì model exposed
#
# üéâ Successfully deployed phi.
# üåê The model has been exposed through a service over 192.168.1.171:32379.
#
# To start a chat with ollama:
#
#  OLLAMA_HOST=192.168.1.171:32379 ollama run phi
#
# To integrate with your OpenAI API compatible client:
#
#  curl http://192.168.1.171:32379/v1/chat/completions -H "Content-Type: application/json" -d '{
#    "model": "phi",
#    "messages": [
#      {
#        "role": "user",
#        "content": "Hello!"
#      }
#    ]
# }'



# kubectl wait --for=jsonpath='{.status.readyReplicas}'=1 deployment/ollama-model-phi -n ollama-operator-system



# OLLAMA_HOST=192.168.1.171:32379 ollama run phi

